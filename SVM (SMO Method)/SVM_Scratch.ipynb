{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scratch - SMO\n",
        "---"
      ],
      "metadata": {
        "id": "eDuLAzhbWW_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SVC:\n",
        "    def __init__(self, C=1.0, tol=1e-3, max_passes=100, gamma=0.1):\n",
        "        \"\"\"\n",
        "        Inisialisasi parameter untuk Support Vector Classifier.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        C : float, default=1.0\n",
        "            Parameter regularisasi. Nilai lebih besar akan meningkatkan kesalahan\n",
        "            margin yang lebih kecil, tetapi dapat menyebabkan overfitting.\n",
        "\n",
        "        tol : float, default=1e-3\n",
        "            Toleransi yang digunakan untuk memutuskan kapan algoritma selesai.\n",
        "\n",
        "        max_passes : int, default=100\n",
        "            Jumlah maksimal iterasi yang dilakukan untuk memverifikasi konvergensi.\n",
        "\n",
        "        gamma : float, default=0.1\n",
        "            Parameter untuk kernel Radial Basis Function (RBF).\n",
        "        \"\"\"\n",
        "        self.C = C  # control threshold for margin\n",
        "        self.tol = tol  # tolerance threshold\n",
        "        self.max_passes = max_passes  # number of iterations\n",
        "        self.gamma = gamma # parameter for RBF kernel\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fitting model\"\"\"\n",
        "        # Convert to numpy arrays\n",
        "        X = np.array(X).copy()\n",
        "        y = np.array(y).copy()\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize parameters\n",
        "        self._initialize_parameters(X)\n",
        "        passes = 0  # Iteration counter\n",
        "\n",
        "        while passes < self.max_passes:\n",
        "            num_changed_alphas = 0\n",
        "\n",
        "            for i in range(n_samples):\n",
        "                # Calculate error\n",
        "                E_i = self._calculate_E(X[i], y[i], X, y)\n",
        "\n",
        "                # Check conditions for optimization\n",
        "                cond_1 = (y[i] * E_i < -self.tol) and (self.alpha[i] < self.C)\n",
        "                cond_2 = (y[i] * E_i > self.tol) and (self.alpha[i] > 0)\n",
        "\n",
        "                if cond_1 or cond_2:\n",
        "                    # Select a second sample randomly\n",
        "                    j = np.random.randint(0, n_samples)\n",
        "\n",
        "                    # Extract data\n",
        "                    X_j, y_j, a_j = X[j], y[j], self.alpha[j]\n",
        "\n",
        "                    # Calculate error for the second sample\n",
        "                    E_j = self._calculate_E(X_j, y_j, X, y)\n",
        "\n",
        "                    # Save old alphas\n",
        "                    old_a_i, old_a_j = np.copy(self.alpha[i]), np.copy(self.alpha[j])\n",
        "\n",
        "                    # Boundary adjustments for alpha\n",
        "                    L, H = self._compute_boundary(y[i], y_j, self.alpha[i], a_j)\n",
        "\n",
        "                    if L == H:  # No change, move to next iteration\n",
        "                        continue\n",
        "\n",
        "                    # Compute eta\n",
        "                    eta = 2 * self._kernel(X[i], X_j) - self._kernel(X[i], X[i]) - self._kernel(X_j, X_j)\n",
        "\n",
        "                    if eta >= 0:  # Skip if eta is not negative\n",
        "                        continue\n",
        "\n",
        "                    # Update alpha_j\n",
        "                    a_j_unclipped = old_a_j - (y_j * (E_i - E_j)) / eta\n",
        "\n",
        "                    # Clip alpha_j to the range [L, H]\n",
        "                    a_j_new = np.clip(a_j_unclipped, L, H)\n",
        "\n",
        "                    if np.abs(a_j_new - old_a_j) < self.tol:\n",
        "                        continue  # No significant change, move to next iteration\n",
        "\n",
        "                    # Update alpha_i\n",
        "                    a_i_new = self.alpha[i] + (y[i] * y_j) * (old_a_j - a_j_new)\n",
        "\n",
        "                    # Compute b_1 and b_2\n",
        "                    b_old = self.intercept_\n",
        "\n",
        "                    # Calculate b_1\n",
        "                    b_1 = b_old - E_i - y[i] * (a_i_new - old_a_i) * np.dot(X[i], X[i]) \\\n",
        "                          - y_j * (a_j_new - old_a_j) * np.dot(X[i], X_j)\n",
        "\n",
        "                    # Calculate b_2\n",
        "                    b_2 = b_old - E_j - y_j * (a_j_new - old_a_j) * np.dot(X_j, X_j) \\\n",
        "                          - y[i] * (a_i_new - old_a_i) * np.dot(X_j, X[i])\n",
        "\n",
        "                    # Set b to b_1, b_2, or average\n",
        "                    if 0 < a_i_new < self.C:\n",
        "                        b_new = b_1\n",
        "                    elif 0 < a_j_new < self.C:\n",
        "                        b_new = b_2\n",
        "                    else:\n",
        "                        b_new = (b_1 + b_2) / 2\n",
        "\n",
        "                    # Update alpha and intercept\n",
        "                    self.alpha[i], self.alpha[j] = a_i_new, a_j_new\n",
        "                    self.intercept_ = b_new\n",
        "                    self._compute_coef(X, y)\n",
        "\n",
        "                    # Increment number of changed alphas\n",
        "                    num_changed_alphas += 1\n",
        "\n",
        "            # Stopping condition\n",
        "            if num_changed_alphas == 0:\n",
        "                passes += 1\n",
        "            else:\n",
        "                passes = 0\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        return np.sign(np.dot(X, self.coef_) + self.intercept_).astype(int)\n",
        "\n",
        "    def _compute_coef(self, X, y):\n",
        "        \"\"\"Compute coefficients\"\"\"\n",
        "        self.coef_ = np.dot((self.alpha * y).T, X)\n",
        "\n",
        "    def _compute_boundary(self, y_i, y_j, a_i, a_j):\n",
        "        \"\"\"Upper and lower boundary for a_j\"\"\"\n",
        "        if y_i != y_j:\n",
        "            L = max(0, a_j - a_i)\n",
        "            H = min(self.C, self.C + a_j - a_i)\n",
        "        else:\n",
        "            L = max(0, a_i + a_j - self.C)\n",
        "            H = min(self.C, a_i + a_j)\n",
        "        return L, H\n",
        "\n",
        "    def _initialize_parameters(self, X):\n",
        "        \"\"\"Initialize parameters before training\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        self.alpha = np.zeros(n_samples)  # Weight for each data point\n",
        "        self.coef_ = np.zeros(n_features)  # Contribution of features for output\n",
        "        self.intercept_ = 0.0  # Hyperplane intercept\n",
        "\n",
        "    def _calculate_E(self, X_star, y_star, X, y):\n",
        "        \"\"\"\n",
        "        Calculate the error (E) between the predicted and true labels.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_star : array-like, shape (n_samples, n_features)\n",
        "            Samples to be predicted.\n",
        "\n",
        "        y_star : array-like, shape (n_samples,)\n",
        "            True labels for `X_star`.\n",
        "\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Training data (not used in error calculation).\n",
        "\n",
        "        y : array-like, shape (n_samples,)\n",
        "            True labels for training data (not used in error calculation).\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        E : array-like, shape (n_samples,)\n",
        "            Prediction error (predicted - true labels).\n",
        "        \"\"\"\n",
        "\n",
        "        # _calculate_F\n",
        "        _calculate_F = np.dot(self.alpha, np.dot(X, X_star.T)) + self.intercept_\n",
        "\n",
        "        # Calculate error\n",
        "        f = _calculate_F  # Get predictions\n",
        "        E = f - y_star  # Calculate error\n",
        "        return E\n",
        "\n",
        "    def _kernel(self, x1, x2):\n",
        "        \"\"\"Compute RBF kernel (Gaussian Kernel)\"\"\"\n",
        "        return np.exp(-self.gamma * np.linalg.norm(x1 - x2) ** 2)"
      ],
      "metadata": {
        "id": "Y5kwV6phdbmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Hanya menggunakan dua kelas untuk klasifikasi biner\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Bagi data menjadi train dan test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the dataset\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "J_gu79cVW6sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scratch\n",
        "---"
      ],
      "metadata": {
        "id": "RusFPUKmXFq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi dan latih model\n",
        "model = SVC(C=1.0, tol=1e-3, gamma=0.1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Prediksi dan evaluasi akurasi\n",
        "y_pred = model.predict(X_train)\n",
        "accuracy = accuracy_score(y_train, y_pred)\n",
        "\n",
        "print(f\"Hasil Prediksi: {y_pred}\")\n",
        "print(f\"Akurasi: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBAGLmDbXFAX",
        "outputId": "fe148478-5135-46d9-b473-5f1afb5cb49c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hasil Prediksi: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0]\n",
            "Akurasi: 47.50%\n"
          ]
        }
      ]
    }
  ]
}